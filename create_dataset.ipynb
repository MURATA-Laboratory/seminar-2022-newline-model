{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改行・読点挿入モデル データセット作成\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE_NAME = \"sidb.db\"\n",
    "TABLE_NAME = \"docs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_FILE_NAME)\n",
    "df = pd.read_sql(f\"SELECT * FROM {TABLE_NAME}\", conn)\n",
    "conn.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "\n",
    "df_check = df.sample(n=3)\n",
    "MAX_ONE_LINE_LENGTH = 100\n",
    "for index in df_check.index:\n",
    "    print(\"\\n\\n\")\n",
    "    for col in df_check.columns:\n",
    "        print(\n",
    "            col,\n",
    "            str(df_check.loc[index, col])[:MAX_ONE_LINE_LENGTH] + \"...\"\n",
    "            if len(str(df_check.loc[index, col])) > MAX_ONE_LINE_LENGTH\n",
    "            else str(df_check.loc[index, col]),\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data to be created\n",
    "\n",
    "```py\n",
    "['id', 'content', 'meta_info', 'sentence', 'clause', 'chunk', 'token', 'lf', 'lfp_lf', 'lfp_p', 'p']\n",
    "```\n",
    "\n",
    "- sentence = 文\n",
    "- clause = 節\n",
    "- chunk = 文節\n",
    "- token = 形態素\n",
    "- lf = 改行 (line feed)\n",
    "- p = 読点 (punctuation)\n",
    "- lfp_lf = 改行 (改行と読点の組み合わせ)\n",
    "- lfp_p = 読点 (改行と読点の組み合わせ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in [1]:  # df.index:\n",
    "#     content: str = df.loc[index, \"content\"]\n",
    "#     sentence_data: list[dict] = json.loads(df.loc[index, \"sentence\"])\n",
    "#     clause_data: list[dict] = json.loads(df.loc[index, \"clause\"])\n",
    "#     chunk_data: list[dict] = json.loads(df.loc[index, \"chunk\"])\n",
    "#     token_data: list[dict] = json.loads(df.loc[index, \"token\"])\n",
    "#     lf_data: list[dict] = json.loads(df.loc[index, \"lf\"])\n",
    "#     lfp_lf_data: list[dict] = json.loads(df.loc[index, \"lfp_lf\"])\n",
    "#     lfp_p_data: list[dict] = json.loads(df.loc[index, \"lfp_p\"])\n",
    "#     # p_data: list[dict]  =  df.loc[index, \"p\"] # データなし\n",
    "\n",
    "#     lfp_lf_data = [x[\"end\"] for x in lfp_lf_data]\n",
    "#     lfp_p_data = [x[\"end\"] for x in lfp_p_data]\n",
    "#     sentence_data = [x[\"end\"] for x in sentence_data]\n",
    "#     for data in chunk_data:\n",
    "#         print(content[data[\"begin\"] : data[\"end\"]], end=\"\")\n",
    "#         is_end_of_sentence = data[\"end\"] in sentence_data\n",
    "#         is_lf = data[\"end\"] in lfp_lf_data\n",
    "#         is_p = data[\"end\"] in lfp_p_data and not is_end_of_sentence\n",
    "#         if is_p:\n",
    "#             print(\"、\", end=\"\")\n",
    "#         if is_end_of_sentence:\n",
    "#             print(\"。\", end=\"\")\n",
    "#         if is_lf:\n",
    "#             print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKEN = \"[ANS]\"\n",
    "COLUMNS = [\"input\", \"is_line_feed\", \"comma_period\"]\n",
    "\n",
    "\n",
    "def create_dataset(df, index, dataset):\n",
    "    content: str = df.loc[index, \"content\"]\n",
    "    chunk_data: list[dict] = json.loads(df.loc[index, \"chunk\"])\n",
    "    sentence_data: list[dict] = json.loads(df.loc[index, \"sentence\"])\n",
    "    lfp_lf_data: list[dict] = json.loads(df.loc[index, \"lfp_lf\"])\n",
    "    lfp_p_data: list[dict] = json.loads(df.loc[index, \"lfp_p\"])\n",
    "\n",
    "    lfp_lf_set = set([x[\"end\"] for x in lfp_lf_data])\n",
    "    lfp_p_set = set([x[\"end\"] for x in lfp_p_data])\n",
    "    sentence_set = set([x[\"end\"] for x in sentence_data])\n",
    "\n",
    "    new_data = []\n",
    "    for i, data in enumerate(chunk_data[:-1]):\n",
    "        is_end_of_sentence = data[\"end\"] in sentence_set\n",
    "        new_data.append(\n",
    "            [\n",
    "                content[data[\"begin\"] : data[\"end\"]]\n",
    "                + SPECIAL_TOKEN\n",
    "                + content[chunk_data[i + 1][\"begin\"] : chunk_data[i + 1][\"end\"]],\n",
    "                int(data[\"end\"] in lfp_lf_set),\n",
    "                # 挿入なし=0, 読点=1, 句点=2\n",
    "                1 if data[\"end\"] in lfp_p_set and not is_end_of_sentence else 2\n",
    "                if is_end_of_sentence\n",
    "                else 0,\n",
    "            ]\n",
    "        )\n",
    "    dataset = pd.DataFrame(\n",
    "        np.vstack((dataset.values, np.array(new_data))), columns=COLUMNS\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# train dataset (all except last)\n",
    "train_dataset = pd.DataFrame(data=None, index=None, columns=COLUMNS)\n",
    "for index in range(len(df) - 1):\n",
    "    train_dataset = create_dataset(df, index, train_dataset)\n",
    "train_dataset.to_csv(\"train_dataset.csv\")\n",
    "\n",
    "# test dataset (last)\n",
    "test_dataset = pd.DataFrame(data=None, index=None, columns=COLUMNS)\n",
    "for index in [len(df) - 1]:\n",
    "    test_dataset = create_dataset(df, index, test_dataset)\n",
    "test_dataset.to_csv(\"test_dataset.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm Created Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_dataset = pd.read_csv(\"test_dataset.csv\")\n",
    "\n",
    "for index in created_dataset.index:\n",
    "    text, _ = created_dataset.loc[index, \"input\"].split(SPECIAL_TOKEN)\n",
    "    print(text, end=\"\")\n",
    "    if created_dataset.loc[index, \"comma_period\"] == 1:\n",
    "        print(\"、\", end=\"\")\n",
    "    if created_dataset.loc[index, \"comma_period\"] == 2:\n",
    "        print(\"。\", end=\"\")\n",
    "    if created_dataset.loc[index, \"is_line_feed\"]:\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "294252db5989f7e12b7215c0f64397d9acb0d211d5f7e00aa1e71c3fd3e5557d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
